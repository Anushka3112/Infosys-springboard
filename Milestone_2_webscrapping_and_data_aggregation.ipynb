{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dae_xieWGYE4",
        "outputId": "94425b35-6649-46a8-ff5d-f32087874cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Collecting pyee<14,>=13 (from playwright)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
            "Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl (46.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "Successfully installed playwright-1.57.0 pyee-13.0.0\n",
            "Downloading Chromium 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-linux.zip\u001b[22m\n",
            "(node:977) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G164.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 0% 68.2s\u001b[0K\u001b[1G164.7 MiB [] 0% 55.8s\u001b[0K\u001b[1G164.7 MiB [] 0% 66.9s\u001b[0K\u001b[1G164.7 MiB [] 0% 51.2s\u001b[0K\u001b[1G164.7 MiB [] 0% 43.5s\u001b[0K\u001b[1G164.7 MiB [] 0% 36.6s\u001b[0K\u001b[1G164.7 MiB [] 0% 30.5s\u001b[0K\u001b[1G164.7 MiB [] 0% 24.1s\u001b[0K\u001b[1G164.7 MiB [] 0% 19.4s\u001b[0K\u001b[1G164.7 MiB [] 1% 15.5s\u001b[0K\u001b[1G164.7 MiB [] 1% 15.7s\u001b[0K\u001b[1G164.7 MiB [] 1% 12.4s\u001b[0K\u001b[1G164.7 MiB [] 1% 12.7s\u001b[0K\u001b[1G164.7 MiB [] 1% 12.5s\u001b[0K\u001b[1G164.7 MiB [] 2% 11.5s\u001b[0K\u001b[1G164.7 MiB [] 2% 10.4s\u001b[0K\u001b[1G164.7 MiB [] 3% 8.4s\u001b[0K\u001b[1G164.7 MiB [] 3% 8.1s\u001b[0K\u001b[1G164.7 MiB [] 4% 7.5s\u001b[0K\u001b[1G164.7 MiB [] 5% 5.9s\u001b[0K\u001b[1G164.7 MiB [] 6% 5.4s\u001b[0K\u001b[1G164.7 MiB [] 7% 4.9s\u001b[0K\u001b[1G164.7 MiB [] 7% 5.0s\u001b[0K\u001b[1G164.7 MiB [] 8% 4.3s\u001b[0K\u001b[1G164.7 MiB [] 9% 4.4s\u001b[0K\u001b[1G164.7 MiB [] 10% 4.3s\u001b[0K\u001b[1G164.7 MiB [] 11% 4.1s\u001b[0K\u001b[1G164.7 MiB [] 11% 3.9s\u001b[0K\u001b[1G164.7 MiB [] 12% 4.0s\u001b[0K\u001b[1G164.7 MiB [] 13% 3.7s\u001b[0K\u001b[1G164.7 MiB [] 14% 3.6s\u001b[0K\u001b[1G164.7 MiB [] 14% 3.7s\u001b[0K\u001b[1G164.7 MiB [] 15% 3.5s\u001b[0K\u001b[1G164.7 MiB [] 16% 3.5s\u001b[0K\u001b[1G164.7 MiB [] 17% 3.5s\u001b[0K\u001b[1G164.7 MiB [] 18% 3.2s\u001b[0K\u001b[1G164.7 MiB [] 19% 3.1s\u001b[0K\u001b[1G164.7 MiB [] 21% 2.9s\u001b[0K\u001b[1G164.7 MiB [] 22% 2.8s\u001b[0K\u001b[1G164.7 MiB [] 23% 2.7s\u001b[0K\u001b[1G164.7 MiB [] 24% 2.7s\u001b[0K\u001b[1G164.7 MiB [] 25% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 26% 2.5s\u001b[0K\u001b[1G164.7 MiB [] 27% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 28% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 29% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 30% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 31% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 32% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 33% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 33% 2.1s\u001b[0K\u001b[1G164.7 MiB [] 34% 2.1s\u001b[0K\u001b[1G164.7 MiB [] 35% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 36% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 37% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 37% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 38% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 39% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 40% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 42% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 43% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 44% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 45% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 46% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 48% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 49% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 50% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 51% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 52% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 53% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 54% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 55% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 56% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 57% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 58% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 59% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 61% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 61% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 62% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 63% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 64% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 65% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 66% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 67% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 68% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 69% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 70% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 71% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 71% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 73% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 74% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 75% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 76% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 77% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 78% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 80% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 81% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 83% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 84% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 85% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 86% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 87% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 88% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 89% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 90% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 91% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 93% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 97% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium-1200\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "(node:1020) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 2% 0.8s\u001b[0K\u001b[1G2.3 MiB [] 4% 0.8s\u001b[0K\u001b[1G2.3 MiB [] 6% 0.9s\u001b[0K\u001b[1G2.3 MiB [] 9% 0.7s\u001b[0K\u001b[1G2.3 MiB [] 13% 0.6s\u001b[0K\u001b[1G2.3 MiB [] 18% 0.5s\u001b[0K\u001b[1G2.3 MiB [] 25% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 33% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 44% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 60% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 79% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 93% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Downloading Chromium Headless Shell 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-headless-shell-linux.zip\u001b[22m\n",
            "(node:1035) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G109.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 0% 45.5s\u001b[0K\u001b[1G109.7 MiB [] 0% 37.7s\u001b[0K\u001b[1G109.7 MiB [] 0% 39.3s\u001b[0K\u001b[1G109.7 MiB [] 0% 31.8s\u001b[0K\u001b[1G109.7 MiB [] 0% 28.0s\u001b[0K\u001b[1G109.7 MiB [] 0% 23.0s\u001b[0K\u001b[1G109.7 MiB [] 0% 19.4s\u001b[0K\u001b[1G109.7 MiB [] 0% 14.8s\u001b[0K\u001b[1G109.7 MiB [] 1% 12.2s\u001b[0K\u001b[1G109.7 MiB [] 1% 9.6s\u001b[0K\u001b[1G109.7 MiB [] 2% 7.6s\u001b[0K\u001b[1G109.7 MiB [] 3% 6.1s\u001b[0K\u001b[1G109.7 MiB [] 4% 5.4s\u001b[0K\u001b[1G109.7 MiB [] 4% 4.8s\u001b[0K\u001b[1G109.7 MiB [] 5% 4.3s\u001b[0K\u001b[1G109.7 MiB [] 6% 4.3s\u001b[0K\u001b[1G109.7 MiB [] 7% 3.7s\u001b[0K\u001b[1G109.7 MiB [] 9% 3.4s\u001b[0K\u001b[1G109.7 MiB [] 9% 3.2s\u001b[0K\u001b[1G109.7 MiB [] 11% 2.9s\u001b[0K\u001b[1G109.7 MiB [] 12% 2.7s\u001b[0K\u001b[1G109.7 MiB [] 13% 2.5s\u001b[0K\u001b[1G109.7 MiB [] 15% 2.3s\u001b[0K\u001b[1G109.7 MiB [] 16% 2.2s\u001b[0K\u001b[1G109.7 MiB [] 17% 2.1s\u001b[0K\u001b[1G109.7 MiB [] 18% 2.0s\u001b[0K\u001b[1G109.7 MiB [] 19% 2.0s\u001b[0K\u001b[1G109.7 MiB [] 21% 1.9s\u001b[0K\u001b[1G109.7 MiB [] 22% 1.8s\u001b[0K\u001b[1G109.7 MiB [] 24% 1.7s\u001b[0K\u001b[1G109.7 MiB [] 25% 1.7s\u001b[0K\u001b[1G109.7 MiB [] 26% 1.6s\u001b[0K\u001b[1G109.7 MiB [] 27% 1.6s\u001b[0K\u001b[1G109.7 MiB [] 28% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 30% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 31% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 32% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 32% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 33% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 34% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 36% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 38% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 40% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 41% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 42% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 43% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 45% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 47% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 48% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 49% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 50% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 51% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 53% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 54% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 55% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 56% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 58% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 59% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 60% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 61% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 62% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 64% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 65% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 67% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 69% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 70% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 72% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 73% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 74% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 75% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 77% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 78% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 80% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 82% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 84% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 85% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 86% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 87% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 89% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1200\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core gsettings-desktop-schemas libatk1.0-data libxtst6\n",
            "  session-migration\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core gsettings-desktop-schemas libatk-bridge2.0-0 libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libxcomposite1 libxtst6 session-migration\n",
            "0 upgraded, 9 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 318 kB of archives.\n",
            "After this operation, 1,497 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Fetched 318 kB in 2s (162 kB/s)\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../1-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../2-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../3-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../4-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../5-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../6-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../7-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../8-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service ‚Üí /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n",
            "\n",
            "üìÑ Detecting total pages...\n",
            "‚úÖ Total pages found: 20\n",
            "\n",
            "üîé Scraping Page 1: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=1\n",
            "\n",
            "üîé Scraping Page 2: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=2\n",
            "\n",
            "üîé Scraping Page 3: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=3\n",
            "\n",
            "üîé Scraping Page 4: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=4\n",
            "\n",
            "üîé Scraping Page 5: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=5\n",
            "\n",
            "üîé Scraping Page 6: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=6\n",
            "\n",
            "üîé Scraping Page 7: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=7\n",
            "\n",
            "üîé Scraping Page 8: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=8\n",
            "\n",
            "üîé Scraping Page 9: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=9\n",
            "\n",
            "üîé Scraping Page 10: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=10\n",
            "\n",
            "üîé Scraping Page 11: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=11\n",
            "\n",
            "üîé Scraping Page 12: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=12\n",
            "\n",
            "üîé Scraping Page 13: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=13\n",
            "\n",
            "üîé Scraping Page 14: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=14\n",
            "\n",
            "üîé Scraping Page 15: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=15\n",
            "\n",
            "üîé Scraping Page 16: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=16\n",
            "\n",
            "üîé Scraping Page 17: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=17\n",
            "\n",
            "üîé Scraping Page 18: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=18\n",
            "\n",
            "üîé Scraping Page 19: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=19\n",
            "\n",
            "üîé Scraping Page 20: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=20\n",
            "\n",
            "üéâ Total products collected: 117\n",
            "\n",
            "üìÅ Saved CSV  ‚Üí output/all_products.csv\n",
            "üìÅ Saved JSON ‚Üí output/all_products.json\n"
          ]
        }
      ],
      "source": [
        "!pip install playwright nest_asyncio\n",
        "!playwright install chromium\n",
        "!apt-get install -y libatk1.0-0 libatk-bridge2.0-0 libatspi2.0-0 libxcomposite1\n",
        "\n",
        "import asyncio, json, csv\n",
        "from pathlib import Path\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "BASE_URL = \"https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page={}\"\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# Detect total pages ‚Äî NO TIMEOUTS USED\n",
        "# =================================================================\n",
        "async def get_total_pages(page):\n",
        "    print(\"\\nüìÑ Detecting total pages...\")\n",
        "\n",
        "    response = await page.goto(BASE_URL.format(1))\n",
        "    if not response or response.status != 200:\n",
        "        print(\"‚ùå Could not load first page. Default pages = 1\")\n",
        "        return 1\n",
        "\n",
        "    try:\n",
        "        await page.wait_for_selector(\".pagination\")\n",
        "    except:\n",
        "        print(\"‚ùå Pagination not found. Default pages = 1\")\n",
        "        return 1\n",
        "\n",
        "    nums = []\n",
        "    buttons = await page.query_selector_all(\".pagination li a\")\n",
        "    for b in buttons:\n",
        "        text = (await b.text_content()).strip()\n",
        "        if text.isdigit():\n",
        "            nums.append(int(text))\n",
        "\n",
        "    total_pages = max(nums) if nums else 1\n",
        "    print(f\"‚úÖ Total pages found: {total_pages}\")\n",
        "    return total_pages\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# Scrape a single page ‚Äî NO TIMEOUTS, NO RETRIES\n",
        "# =================================================================\n",
        "async def scrape_page(page, page_number):\n",
        "    url = BASE_URL.format(page_number)\n",
        "    print(f\"\\nüîé Scraping Page {page_number}: {url}\")\n",
        "\n",
        "    # Try opening the page\n",
        "    response = await page.goto(url)\n",
        "\n",
        "    # If URL not found / server error\n",
        "    if not response:\n",
        "        print(\"‚ùå No response. Skipping page.\")\n",
        "        return []\n",
        "\n",
        "    if response.status != 200:\n",
        "        print(f\"‚ùå Bad status {response.status}. Skipping page.\")\n",
        "        return []\n",
        "\n",
        "    # Try selecting product cards\n",
        "    try:\n",
        "        await page.wait_for_selector(\".thumbnail\")\n",
        "    except:\n",
        "        print(\"‚ùå No products found. Skipping page.\")\n",
        "        return []\n",
        "\n",
        "    cards = await page.query_selector_all(\".thumbnail\")\n",
        "    items = []\n",
        "\n",
        "    for card in cards:\n",
        "        title_el = await card.query_selector(\".title\")\n",
        "        price_el = await card.query_selector(\".price\")\n",
        "        img_el   = await card.query_selector(\"img\")\n",
        "\n",
        "        title = (await title_el.text_content()).strip() if title_el else None\n",
        "        price = (await price_el.text_content()).strip() if price_el else None\n",
        "        image = await img_el.get_attribute(\"src\") if img_el else None\n",
        "\n",
        "        stars = await card.query_selector_all(\".glyphicon-star\")\n",
        "        rating = len(stars)\n",
        "\n",
        "        items.append({\n",
        "            \"title\": title,\n",
        "            \"price\": price,\n",
        "            \"rating_stars\": rating,\n",
        "            \"image_url\": image,\n",
        "            \"page\": page_number\n",
        "        })\n",
        "\n",
        "    return items\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# Scrape all pages\n",
        "# =================================================================\n",
        "async def scrape_all():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        total_pages = await get_total_pages(page)\n",
        "        all_items = []\n",
        "\n",
        "        for page_no in range(1, total_pages + 1):\n",
        "            items = await scrape_page(page, page_no)\n",
        "            all_items.extend(items)\n",
        "\n",
        "        await browser.close()\n",
        "        return all_items\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# Run scraper\n",
        "# =================================================================\n",
        "data = asyncio.get_event_loop().run_until_complete(scrape_all())\n",
        "print(f\"\\nüéâ Total products collected: {len(data)}\")\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# Save results\n",
        "# =================================================================\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "\n",
        "csv_path = Path(\"output/all_products.csv\")\n",
        "json_path = Path(\"output/all_products.json\")\n",
        "\n",
        "# Save CSV\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "# Save JSON\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nüìÅ Saved CSV  ‚Üí\", csv_path)\n",
        "print(\"üìÅ Saved JSON ‚Üí\", json_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio, json, csv\n",
        "from pathlib import Path\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "MAIN_URL = \"https://webscraper.io/test-sites/e-commerce/static\"\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# (1) Detect all sections (NO TIMEOUT, NO RETRY)\n",
        "# =====================================================================\n",
        "async def detect_all_sections(page):\n",
        "    response = await page.goto(MAIN_URL)\n",
        "    if not response or response.status != 200:\n",
        "        raise Exception(\"‚ùå Could not load main page\")\n",
        "\n",
        "    try:\n",
        "        await page.wait_for_selector(\".category-link\")\n",
        "    except:\n",
        "        raise Exception(\"‚ùå Section links not found\")\n",
        "\n",
        "    sections = {}\n",
        "    links = await page.locator(\".category-link\").all()\n",
        "\n",
        "    for link in links:\n",
        "        name = (await link.text_content()).strip().lower()\n",
        "        url = await link.get_attribute(\"href\")\n",
        "        if url.startswith(\"/\"):\n",
        "            url = \"https://webscraper.io\" + url\n",
        "        sections[name] = url\n",
        "\n",
        "    print(\"üìå Sections Found:\", sections)\n",
        "    return sections\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# (2) Detect subsections (NO TIMEOUT, NO RETRY)\n",
        "# =====================================================================\n",
        "async def detect_subsections(page, section_url):\n",
        "\n",
        "    response = await page.goto(section_url)\n",
        "    if not response or response.status != 200:\n",
        "        raise Exception(\"‚ùå Could not load section page\")\n",
        "\n",
        "    try:\n",
        "        await page.wait_for_selector(\".subcategory-link\")\n",
        "    except:\n",
        "        print(\"‚ùå No subsections found\")\n",
        "        return {}\n",
        "\n",
        "    subs = {}\n",
        "    links = await page.locator(\".subcategory-link\").all()\n",
        "\n",
        "    for sl in links:\n",
        "        name = (await sl.text_content()).strip().lower()\n",
        "        url = await sl.get_attribute(\"href\")\n",
        "        if url.startswith(\"/\"):\n",
        "            url = \"https://webscraper.io\" + url\n",
        "        subs[name] = url\n",
        "\n",
        "    print(\"üìÇ Subsections Found:\", subs)\n",
        "    return subs\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# (3) Get total page count\n",
        "# =====================================================================\n",
        "async def get_total_pages(page):\n",
        "    try:\n",
        "        buttons = await page.locator(\"ul.pagination li a\").all()\n",
        "    except:\n",
        "        return 1\n",
        "\n",
        "    nums = []\n",
        "    for b in buttons:\n",
        "        t = (await b.text_content()).strip()\n",
        "        if t.isdigit():\n",
        "            nums.append(int(t))\n",
        "\n",
        "    return max(nums) if nums else 1\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# (4) Scrape one page (NO TIMEOUT)\n",
        "# =====================================================================\n",
        "async def scrape_page(page, url):\n",
        "\n",
        "    response = await page.goto(url)\n",
        "    if not response or response.status != 200:\n",
        "        print(f\"‚ùå Skipping (bad URL): {url}\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        await page.wait_for_selector(\".thumbnail\")\n",
        "    except:\n",
        "        print(f\"‚ùå No products found on page: {url}\")\n",
        "        return []\n",
        "\n",
        "    cards = await page.locator(\".thumbnail\").all()\n",
        "    products = []\n",
        "\n",
        "    for c in cards:\n",
        "        title = await c.locator(\".title\").text_content()\n",
        "        price = await c.locator(\".price\").text_content()\n",
        "        img = await c.locator(\"img\").get_attribute(\"src\")\n",
        "        link = await c.locator(\".title\").get_attribute(\"href\")\n",
        "        stars = await c.locator(\".glyphicon-star\").count()\n",
        "\n",
        "        products.append({\n",
        "            \"title\": title.strip(),\n",
        "            \"price\": price.strip(),\n",
        "            \"rating\": stars,\n",
        "            \"image_url\": img,\n",
        "            \"product_url\": link\n",
        "        })\n",
        "\n",
        "    return products\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# (5) Main scraper (Only Tablets)\n",
        "# =====================================================================\n",
        "async def scrape_tablets():\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        # Step 1: detect \"computers\" section\n",
        "        sections = await detect_all_sections(page)\n",
        "\n",
        "        comp_url = sections.get(\"computers\")\n",
        "        if not comp_url:\n",
        "            raise Exception(\"‚ùå Computers section not found\")\n",
        "\n",
        "        # Step 2: detect subsections inside computers\n",
        "        subs = await detect_subsections(page, comp_url)\n",
        "\n",
        "        if \"tablets\" not in subs:\n",
        "            raise Exception(\"‚ùå Tablets section not found\")\n",
        "\n",
        "        tablets_url = subs[\"tablets\"]\n",
        "        print(\"üìå Tablets URL:\", tablets_url)\n",
        "\n",
        "        # Step 3: detect total pages\n",
        "        resp = await page.goto(tablets_url)\n",
        "        if not resp or resp.status != 200:\n",
        "            raise Exception(\"‚ùå Cannot open tablets URL to detect pages\")\n",
        "\n",
        "        total_pages = await get_total_pages(page)\n",
        "        print(f\"üìÑ Total tablet pages: {total_pages}\")\n",
        "\n",
        "        # Step 4: scrap all tablet pages\n",
        "        all_tablets = []\n",
        "\n",
        "        for pnum in range(1, total_pages + 1):\n",
        "            url = f\"{tablets_url}?page={pnum}\"\n",
        "            print(\"üîé Scraping:\", url)\n",
        "            products = await scrape_page(page, url)\n",
        "            all_tablets.extend(products)\n",
        "\n",
        "        await browser.close()\n",
        "        return all_tablets\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# (6) Run Script & Save output\n",
        "# =====================================================================\n",
        "data = asyncio.get_event_loop().run_until_complete(scrape_tablets())\n",
        "print(f\"\\n‚úÖ Total tablet products collected: {len(data)}\")\n",
        "\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "csv_path = Path(\"output/tablets.csv\")\n",
        "\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(\"üìÅ File saved ‚Üí\", csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3H_FA70GZ-T",
        "outputId": "96e372e9-99cb-4dd3-9c68-1ef9151c5ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìå Sections Found: {'computers': 'https://webscraper.io/test-sites/e-commerce/static/computers', 'phones': 'https://webscraper.io/test-sites/e-commerce/static/phones'}\n",
            "üìÇ Subsections Found: {'laptops': 'https://webscraper.io/test-sites/e-commerce/static/computers/laptops', 'tablets': 'https://webscraper.io/test-sites/e-commerce/static/computers/tablets'}\n",
            "üìå Tablets URL: https://webscraper.io/test-sites/e-commerce/static/computers/tablets\n",
            "üìÑ Total tablet pages: 4\n",
            "üîé Scraping: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=1\n",
            "üîé Scraping: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=2\n",
            "üîé Scraping: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=3\n",
            "üîé Scraping: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=4\n",
            "\n",
            "‚úÖ Total tablet products collected: 21\n",
            "üìÅ File saved ‚Üí output/tablets.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import math\n",
        "\n",
        "# ------------------------------\n",
        "# CONFIGURATION\n",
        "# ------------------------------\n",
        "API_KEY = \"cef5755cba525a47429f31c618317d2a\"  # Replace with your ScraperAPI key\n",
        "COUNTRY = \"in\"\n",
        "TLD = \"com\"\n",
        "\n",
        "# ------------------------------\n",
        "# FUNCTIONS\n",
        "# ------------------------------\n",
        "def fetch_search_results(keyword, page=1):\n",
        "    \"\"\"Fetch Amazon search results from ScraperAPI structured endpoint.\"\"\"\n",
        "    url = \"https://api.scraperapi.com/structured/amazon/search\"\n",
        "    params = {\n",
        "        \"api_key\": API_KEY,\n",
        "        \"query\": keyword,\n",
        "        \"country\": COUNTRY,\n",
        "        \"tld\": TLD,\n",
        "        \"page\": page\n",
        "    }\n",
        "    try:\n",
        "        r = requests.get(url, params=params)\n",
        "        r.raise_for_status()\n",
        "        return r.json()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching page {page} for '{keyword}': {e}\")\n",
        "        return None\n",
        "\n",
        "def save_json(data, filename):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "    print(f\"‚úî JSON saved: {filename}\")\n",
        "\n",
        "def save_data_pandas(products, csv_filename, excel_filename):\n",
        "    df = pd.DataFrame(products)\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    df.to_excel(excel_filename, index=False)\n",
        "    print(f\"‚úî CSV saved: {csv_filename}\")\n",
        "    print(f\"‚úî Excel saved: {excel_filename}\")\n",
        "\n",
        "# ------------------------------\n",
        "# MAIN SCRAPER\n",
        "# ------------------------------\n",
        "def scrape_keyword(keyword):\n",
        "    print(f\"\\nüîç Searching for: '{keyword}'\")\n",
        "\n",
        "    page = 1\n",
        "    all_products = []\n",
        "\n",
        "    # First request\n",
        "    data = fetch_search_results(keyword, page)\n",
        "    if not data:\n",
        "        print(\"No data returned from API.\")\n",
        "        return [], 0, 0\n",
        "\n",
        "    results = data.get(\"results\", [])\n",
        "    products_per_page = len(results) if results else 1\n",
        "\n",
        "    # Add first page\n",
        "    for product in results:\n",
        "        product[\"keyword\"] = keyword\n",
        "    all_products.extend(results)\n",
        "    print(f\"Page {page} scraped: {len(results)} products\")\n",
        "\n",
        "    # Iterate remaining pages until no more results\n",
        "    page += 1\n",
        "    while True:\n",
        "        page_data = fetch_search_results(keyword, page)\n",
        "        if not page_data:\n",
        "            break\n",
        "        page_results = page_data.get(\"results\", [])\n",
        "        if not page_results:\n",
        "            break\n",
        "        for product in page_results:\n",
        "            product[\"keyword\"] = keyword\n",
        "        all_products.extend(page_results)\n",
        "        print(f\"Page {page} scraped: {len(page_results)} products\")\n",
        "        page += 1\n",
        "\n",
        "    # Calculate totals\n",
        "    total_products = len(all_products)\n",
        "    total_pages = math.ceil(total_products / products_per_page) if products_per_page else 1\n",
        "\n",
        "    return all_products, total_products, total_pages\n",
        "\n",
        "# ------------------------------\n",
        "# USER INPUT\n",
        "# ------------------------------\n",
        "keyword_input = input(\"Enter the product keyword to search: \").strip()\n",
        "if not keyword_input:\n",
        "    print(\"No keyword entered. Exiting...\")\n",
        "    exit()\n",
        "\n",
        "# Scrape\n",
        "products_data, total_products, total_pages = scrape_keyword(keyword_input)\n",
        "\n",
        "# ------------------------------\n",
        "# SAVE FILES\n",
        "# ------------------------------\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "save_json(products_data, \"output/amazon_products.json\")\n",
        "save_data_pandas(products_data, \"output/amazon_products.csv\", \"output/amazon_products.xlsx\")\n",
        "\n",
        "# ------------------------------\n",
        "# SUMMARY\n",
        "# ------------------------------\n",
        "print(\"\\n‚úÖ Scraping completed!\")\n",
        "print(f\"Keyword: '{keyword_input}' | Total Products: {total_products} | Total Pages: {total_pages}\")\n",
        "print(\"Products data saved in 'output/' folder\")\n"
      ],
      "metadata": {
        "id": "tJq3VSc3d5Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549d8d9e-fe8a-49ca-8d6b-558228626183"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the product keyword to search: iphone\n",
            "\n",
            "üîç Searching for: 'iphone'\n",
            "Page 1 scraped: 17 products\n",
            "Page 2 scraped: 17 products\n",
            "Page 3 scraped: 17 products\n",
            "Page 4 scraped: 16 products\n",
            "Page 5 scraped: 17 products\n",
            "Page 6 scraped: 16 products\n",
            "Page 7 scraped: 17 products\n",
            "Page 8 scraped: 7 products\n",
            "‚úî JSON saved: output/amazon_products.json\n",
            "‚úî CSV saved: output/amazon_products.csv\n",
            "‚úî Excel saved: output/amazon_products.xlsx\n",
            "\n",
            "‚úÖ Scraping completed!\n",
            "Keyword: 'iphone' | Total Products: 124 | Total Pages: 8\n",
            "Products data saved in 'output/' folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import re\n",
        "import json\n",
        "\n",
        "BASE_URL = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
        "\n",
        "# -------------------------------\n",
        "# Convert rating text to number\n",
        "# -------------------------------\n",
        "def rating_to_number(rating_text):\n",
        "    mapping = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n",
        "    return mapping.get(rating_text, 0)\n",
        "\n",
        "# -------------------------------\n",
        "# Simple sentiment analysis\n",
        "# -------------------------------\n",
        "def analyze_sentiment(description):\n",
        "    positive_keywords = [\"excellent\", \"amazing\", \"best\", \"masterpiece\", \"fantastic\"]\n",
        "    negative_keywords = [\"boring\", \"dull\", \"bad\", \"poor\", \"weak\"]\n",
        "\n",
        "    score = 0\n",
        "    desc_lower = description.lower()\n",
        "    for word in positive_keywords:\n",
        "        if word in desc_lower:\n",
        "            score += 0.2\n",
        "    for word in negative_keywords:\n",
        "        if word in desc_lower:\n",
        "            score -= 0.2\n",
        "\n",
        "    return max(min(score, 1), -1)\n",
        "\n",
        "# -------------------------------\n",
        "# Smart pricing strategy\n",
        "# -------------------------------\n",
        "def pricing_strategy(price, stock, rating, description):\n",
        "    adjusted_price = price\n",
        "    sentiment_score = analyze_sentiment(description)\n",
        "\n",
        "    # 5-star book: increase price slightly\n",
        "    if rating == 5:\n",
        "        adjusted_price *= 1.07  # +7%\n",
        "\n",
        "    # Low stock: increase price slightly\n",
        "    if stock <= 5:\n",
        "        adjusted_price *= 1.05  # +5%\n",
        "\n",
        "    # Positive sentiment: small boost\n",
        "    if sentiment_score > 0.3:\n",
        "        adjusted_price *= 1 + sentiment_score / 10\n",
        "\n",
        "    return round(adjusted_price, 2)\n",
        "\n",
        "# -------------------------------\n",
        "# Scrape one page\n",
        "# -------------------------------\n",
        "def scrape_page(page_number):\n",
        "    url = BASE_URL.format(page_number)\n",
        "    r = requests.get(url)\n",
        "    if r.status_code != 200:\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    books = soup.select(\"article.product_pod\")\n",
        "    data = []\n",
        "\n",
        "    for book in books:\n",
        "        title = book.h3.a[\"title\"]\n",
        "\n",
        "        # Price\n",
        "        price_text = book.select_one(\".price_color\").text.strip()\n",
        "        price = float(re.sub(r\"[^0-9.]\", \"\", price_text))\n",
        "\n",
        "        # Rating\n",
        "        rating_text = book.p[\"class\"][1]\n",
        "        rating = rating_to_number(rating_text)\n",
        "\n",
        "        # Detail page URL\n",
        "        book_url = book.h3.a[\"href\"]\n",
        "        detail_page = \"https://books.toscrape.com/catalogue/\" + book_url\n",
        "\n",
        "        # Default values\n",
        "        stock = 0\n",
        "        description = \"\"\n",
        "\n",
        "        # Fetch detail page for real stock and description\n",
        "        detail_resp = requests.get(detail_page)\n",
        "        if detail_resp.status_code == 200:\n",
        "            detail_soup = BeautifulSoup(detail_resp.text, \"html.parser\")\n",
        "\n",
        "            # Description\n",
        "            desc_tag = detail_soup.select_one(\"#product_description ~ p\")\n",
        "            if desc_tag:\n",
        "                description = desc_tag.text.strip()\n",
        "\n",
        "            # Real stock from table\n",
        "            table_rows = detail_soup.select(\"table.table.table-striped tr\")\n",
        "            for row in table_rows:\n",
        "                th = row.th.text.strip()\n",
        "                td = row.td.text.strip()\n",
        "                if th == \"Availability\":\n",
        "                    match = re.search(r\"\\((\\d+)\\s+available\\)\", td)\n",
        "                    if match:\n",
        "                        stock = int(match.group(1))\n",
        "                    else:\n",
        "                        # fallback if number not shown\n",
        "                        stock = 1\n",
        "                    break\n",
        "\n",
        "        # Adjust price based on rating, stock, and description sentiment\n",
        "        adjusted_price = pricing_strategy(price, stock, rating, description)\n",
        "\n",
        "        # Flags\n",
        "        hot_selling = rating >= 4 and stock <= 5\n",
        "        last_copy = stock == 1\n",
        "\n",
        "        data.append({\n",
        "            \"title\": title,\n",
        "            \"original_price\": price,\n",
        "            \"adjusted_price\": adjusted_price,\n",
        "            \"rating\": rating,\n",
        "            \"stock\": stock,\n",
        "            \"hot_selling\": hot_selling,\n",
        "            \"last_copy\": last_copy,\n",
        "            \"description\": description,\n",
        "            \"url\": book_url\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "# -------------------------------\n",
        "# Scrape all pages\n",
        "# -------------------------------\n",
        "all_books = []\n",
        "page = 1\n",
        "while True:\n",
        "    books = scrape_page(page)\n",
        "    if not books:\n",
        "        break\n",
        "    all_books.extend(books)\n",
        "    print(f\"Page {page} scraped: {len(books)} books\")\n",
        "    page += 1\n",
        "\n",
        "print(f\"\\nTotal books scraped: {len(all_books)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Save data\n",
        "# -------------------------------\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "df = pd.DataFrame(all_books)\n",
        "df.to_csv(\"output/books_prices.csv\", index=False)\n",
        "df.to_excel(\"output/books_prices.xlsx\", index=False)\n",
        "with open(\"output/books_prices.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(all_books, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"üìÅ Saved CSV, Excel, and JSON files in 'output/' folder\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYl_PWzLPmP_",
        "outputId": "abf75ecc-4fa2-4504-be2e-ae51b39a160c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1 scraped: 20 books\n",
            "Page 2 scraped: 20 books\n",
            "Page 3 scraped: 20 books\n",
            "Page 4 scraped: 20 books\n",
            "Page 5 scraped: 20 books\n",
            "Page 6 scraped: 20 books\n",
            "Page 7 scraped: 20 books\n",
            "Page 8 scraped: 20 books\n",
            "Page 9 scraped: 20 books\n",
            "Page 10 scraped: 20 books\n",
            "Page 11 scraped: 20 books\n",
            "Page 12 scraped: 20 books\n",
            "Page 13 scraped: 20 books\n",
            "Page 14 scraped: 20 books\n",
            "Page 15 scraped: 20 books\n",
            "Page 16 scraped: 20 books\n",
            "Page 17 scraped: 20 books\n",
            "Page 18 scraped: 20 books\n",
            "Page 19 scraped: 20 books\n",
            "Page 20 scraped: 20 books\n",
            "Page 21 scraped: 20 books\n",
            "Page 22 scraped: 20 books\n",
            "Page 23 scraped: 20 books\n",
            "Page 24 scraped: 20 books\n",
            "Page 25 scraped: 20 books\n",
            "Page 26 scraped: 20 books\n",
            "Page 27 scraped: 20 books\n",
            "Page 28 scraped: 20 books\n",
            "Page 29 scraped: 20 books\n",
            "Page 30 scraped: 20 books\n",
            "Page 31 scraped: 20 books\n",
            "Page 32 scraped: 20 books\n",
            "Page 33 scraped: 20 books\n",
            "Page 34 scraped: 20 books\n",
            "Page 35 scraped: 20 books\n",
            "Page 36 scraped: 20 books\n",
            "Page 37 scraped: 20 books\n",
            "Page 38 scraped: 20 books\n",
            "Page 39 scraped: 20 books\n",
            "Page 40 scraped: 20 books\n",
            "Page 41 scraped: 20 books\n",
            "Page 42 scraped: 20 books\n",
            "Page 43 scraped: 20 books\n",
            "Page 44 scraped: 20 books\n",
            "Page 45 scraped: 20 books\n",
            "Page 46 scraped: 20 books\n",
            "Page 47 scraped: 20 books\n",
            "Page 48 scraped: 20 books\n",
            "Page 49 scraped: 20 books\n",
            "Page 50 scraped: 20 books\n",
            "\n",
            "Total books scraped: 1000\n",
            "üìÅ Saved CSV, Excel, and JSON files in 'output/' folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQlL8GSSSx6I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}